{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Fundamentals for Data Science: Final Exam\n",
    "### Summer 2016\n",
    "\n",
    "\n",
    "## Instructions\n",
    "The final exam is designed to evaluate your grasp of Python theory as well as Python coding.\n",
    "\n",
    "- This is an individual exam.\n",
    "- You have 24 hours to complete the exam, starting from the point at which you first access it.\n",
    "- You will be graded on the quality of your answers.  Use clear, persuasive arguments based on concepts we covered in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Questions (21 pts )\n",
    "\n",
    "1) The following method is part of a larger program used by a mobile phone company.  It will work when an object of type MobileDevice or of type ServiceContract is passed in.  This is a demonstration of (select all that apply):\n",
    "\n",
    "    1. Inheritance\n",
    "    2. Polymorphism\n",
    "    3. Duck typing\n",
    "    4. Top-down design\n",
    "    5. Functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_to_cart(item):\n",
    "    cart.append(item)\n",
    "    total += item.price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "Above example is a demostration of:\n",
    "2. Polymorphism\n",
    "3. Duck typing\n",
    "5. Functional programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Suppose you have a long list of digits (0-9) that you want to write to a file.  Would it be more efficient to use ASCII or UTF-8 as an encoding?  How could you create an even smaller binary file to store the information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "It would be more efficient to use UTF-8 for encoding as UTF-8 is widely used and also the method Python3 uses. \n",
    "After encoding the list to binary type (.encode('utf-8')), use write() function to write to a binary file, with 'rb' as the argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) You are part of a team working on a spreadsheet program that is written in Python 3.  The program includes several classes to represent different types of objects that fit into a cell of a spreadsheet.  Give a strong argument for why your team should write an abstract base class to respresent such objects, and give examples of what should go into such an abstract base class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "As the cell of a spreadsheet can store different types of objects, it's better to create an abstract base class first and then subclasses for each type of objects with more attributes. Using the base class, it's easier and faster to create subclasses with flexibility. If some changes need to be made across all objects, then perhaps only one change is made in the base class. Also Python doesn't check the type of objects, so you can use the same function for different type of objects.\n",
    "\n",
    "For example, the base class may include attributes 'type' and 'value' and write() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Explain why NumPy is better than lists for \"vectorized\" math operations. Give an example of an operation that is either impossible or painful to implement using tradtional Python lists compared to NumPy arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "NumPy has great ability to work with multi-dimentional arrays and hides the details from end users for common array or vector math operations.\n",
    "\n",
    "For example, it's easy to time each item by 2 in an array in numpy but you have to use loop using list.\n",
    "\n",
    "a = np.arange(10)\n",
    "\n",
    "a * 2\n",
    "\n",
    "Another example is using numpy to create two 3 by 2\n",
    "arrays and add each items of two arrays respectively to create a third 3 by 2 array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) We want a list of the numbers that are the square of nonnegative integer less than 10, but whose squares are greater than 10.  The list comprehension below gives an empty list.  Correct it so that we get the desired output, [16, 25, 36, 49, 64, 81]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 25, 36, 49, 64, 81]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ x**2 for x in range(1,10) if x**2 > 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Explain why the following code prints what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'function'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "pass the function means defining an abstract and empty function and no errors throw out. Even though it's an empty function, f is still a function object in Python. Thus the type of f is 'function'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Explain why the following code prints something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "def f(): pass\n",
    "print(type(f()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Cynthia Hu]:**\n",
    "f() means callling the function. As it's an empty function, it returns 'None' which is a Python type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integrity (25 pts)\n",
    "\n",
    "1) Why is it important to sanity-check your data before you begin your analysis? What could happen if you don't?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "It's import to sanity-check the data before begin analysis. If something is wrong or missing in the dataset, you may detect it later in the analysis and you have to redo the work. What's even worse, if you don't detect the issues, you will get the wrong analysis and conclusions.\n",
    "\n",
    "Common sanity check I would do include followings:\n",
    "0. Overview of the dataset, such as data type and size\n",
    "1. count how many records and distinct count some variables. For example, if I expect one row for one customer, then I would get the same numbers of counting rows and counting distinct customers. Also, using count and distinct count, I would know whether I have a unique key in my table.\n",
    "2. check range of some variables, like the date range\n",
    "3. check missing values\n",
    "4. check outliers or abnormal distribution for some variables, such as 90% are zeros for 'Amount'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Explain, in your own words, why real-world data is often messy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "There are several reasons I could think of for the messy data in the real-word:\n",
    "1. Upstream data may be from user input. The user may not understand the system or which values should be entered or didn't follow the standard process. Or sometimes they just enter some values to pass the system as it's required. However, these values may not reflect the truth and invalid for further analysis.\n",
    "\n",
    "2. There are a lot of changes in the upstream data sources and different systems/tables are not synced. Then you will notice a lot of variances between sources and spend much time investigating.\n",
    "\n",
    "3. There are multiple layers in the data warehouse. Different rules are applied during the process and there is no master data dictionary or document. To answer some business questions, you may find several relevant tables but it's difficult to decide which one is correct. Or you don't know how data was processed and you may misinterprate the data you have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) How do you determine which variables in your dataset you should check for issues prior to starting an analysis? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "I would first have an overview of the dataset, like head(), describe(), dtypes. If some variables are not relevant or duplicates, I would select fewer variables.\n",
    "\n",
    "Next, I will check the key whether it's unique and not null.\n",
    "Then, I will check variables quite relevant to my questions. For example, I'm analyzing the revenue by customer and product type. I will check the distribution of revenue, product type, customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) How do you know when you have adequately checked these variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "It's difficult to say that I have adequately checked these variables. I may miss something but detect it later during exploring and analysis process. And then I would look into the issues further.\n",
    "\n",
    "During the sanity check, I may check the same variables from different perspective. If the conclusions are consistent or reasonable, I would say I can move to the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Is it possible to fully vet your data for errors before you begin your analysis? If not, what should you be looking out for while you complete your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "As I explained above, it's impossible to fully investigate the data for any errors before begin analysis. This is a repeatable process, not just one way. If initial sanity check is passed, I would move on or keep in mind some potential issues.\n",
    "\n",
    "After exploring the dataset further, I may answer some questions I had before or really found some issues with the dataset. Then I have to make decisions on how to process the data, such as removing nulls or replace them with zeros. Finally, once complete the analysis, do my conclusions make sense or meet my expection or contradict the data itself?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elections (24 pts)\n",
    "\n",
    "Consider the following data frame in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>delegates</th>\n",
       "      <th>color</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>marco</td>\n",
       "      <td>165</td>\n",
       "      <td>blue</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jeb</td>\n",
       "      <td>0</td>\n",
       "      <td>red</td>\n",
       "      <td>FL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chris</td>\n",
       "      <td>0</td>\n",
       "      <td>white</td>\n",
       "      <td>NJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>donald</td>\n",
       "      <td>1543</td>\n",
       "      <td>white</td>\n",
       "      <td>NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ted</td>\n",
       "      <td>559</td>\n",
       "      <td>blue</td>\n",
       "      <td>TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>john</td>\n",
       "      <td>161</td>\n",
       "      <td>red</td>\n",
       "      <td>OH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  delegates  color state\n",
       "0   marco        165   blue    FL\n",
       "1     jeb          0    red    FL\n",
       "2   chris          0  white    NJ\n",
       "3  donald       1543  white    NY\n",
       "4     ted        559   blue    TX\n",
       "5    john        161    red    OH"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "# creating a data frame from scratch - list of lists\n",
    "\n",
    "data = [ ['marco', 165, 'blue', 'FL'], \n",
    "         ['jeb', 0, 'red', 'FL'], \n",
    "         ['chris', 0, 'white', 'NJ'], \n",
    "         ['donald', 1543, 'white', 'NY'],\n",
    "         ['ted', 559, 'blue', 'TX'],\n",
    "         ['john', 161, 'red', 'OH']\n",
    "       ]\n",
    "\n",
    "# create a data frame with column names - list of lists\n",
    "\n",
    "col_names = ['name', 'delegates', 'color', 'state']\n",
    "df = pandas.DataFrame(data, columns=col_names)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Using bracket indexing in Pandas, show how many delegates `ted` got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    559\n",
       "Name: delegates, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['name'] == 'ted']['delegates']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Using bracket indexing in Pandas, show how many total delegates were obtained by candidates whose favorite color is blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "724"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = df[df['color'] == 'blue']\n",
    "temp.delegates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Using groupby and aggregate in Pandas, show how many total delegates were obtained by candidates grouped by favorite color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "color\n",
       "blue      724\n",
       "red       161\n",
       "white    1543\n",
       "Name: delegates, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('color').delegates.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clinical disease data (30 pts)\n",
    "\n",
    "Your boss comes to you Monday morning and says “I figured out our next step; we are going to pivot from an online craft store and become a data center for genetic disease information! I found **ClinVar** which is a repository that contains expert curated data, and it is free for the taking. This is a gold mine! Take a week and **<u>tell me what gene and mutation combinations are classified as dangerous.”</u>**\n",
    "\n",
    "1)  Look a the data set and develop a plan of action to use python to extract and summarize just what your boss wants. **Don’t code**. You can use pseudocode and/or and essay format to generate a plan in 500 words or less. \n",
    "\n",
    "2) Tell us the output that you expect from your planned code\n",
    "\n",
    "**Hints:**  \n",
    "\n",
    "* Look at the file carefully. What fields do you want to extract? Are they in the same place every time? What strategy will you use to robustly extract and filter your data of interest? How do you plan to handle missing data?\n",
    "\n",
    "* Filter out junk. Just focus on what your boss asked for (1) gene name (2) mutation reference. (3) Filter your data to include only mutations that are dangerous as you define it. \n",
    "\n",
    "* Pandas and NumPy parsers correctly recognize the end of each line in in the ClinVar file.\n",
    "\n",
    "* The unit of observation of this dataset is one row per mutation.\n",
    "\n",
    "* While you shouldn't code your analysis, creating a few lines of code while you think through the problem may be helpful (so that you can sanity check that your plan works). So you can experiment, we have included the data file below as a Tab Separated Value file \"Genomics_Questions.txt\". Please do not submit any such code. For example, if I wanted to check that I accurately understand the \"split\" function in the context of this data, I could type:\n",
    "\n",
    "```python\n",
    "sample = \"abc;def;asd\"\n",
    "test = sample.split(';')\n",
    "```\n",
    "\n",
    "**This is a planning question we want you to lay out a plan in text not code.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### VCF file description (Summarized from version 4.1)\n",
    "\n",
    "```\n",
    "* The VCF specification:\n",
    "\n",
    "VCF is a text file format which contains meta-information lines, a header\n",
    "line, and then data lines each containing information about a position in the genome. The format also has the ability to contain genotype information on samples for each position.\n",
    "\n",
    "* Fixed fields:\n",
    "\n",
    "There are 4 fixed fields per record. All data lines are **tab-delimited**. In all cases, missing values are specified with a dot (‘.’). \n",
    "\n",
    "1. CHROM - chromosome number\n",
    "2. POS - position DNA nuceleotide count (bases) along the chromosome\n",
    "3. ID - The unique identifier for each mutation\n",
    "4. INFO - a semicolon-separated series of  keys with values in the format: <key>=<data>, and specified as <key>=<data name>[data value definition].\n",
    "\n",
    "```\n",
    "### INFO field specifications\n",
    "\n",
    "```\n",
    "GENEINFO = <Gene symbol>\n",
    "CLNSIG =  <Variant Clinical Significance (Severity)>[0 - Uncertain significance, 2 - Benign, 5 - Pathogenic, 255 - other]\n",
    "\n",
    "```\n",
    "\n",
    "### Representative ClinVar data (vcf file format)\n",
    "\n",
    "```\n",
    "##fileformat=VCFv4.0\n",
    "##fileDate=20160705\n",
    "##source=ClinVar and dbSNP\n",
    "##dbSNP_BUILD_ID=147\n",
    "#CHROM\tPOS\tID\tINFO\n",
    "1\t949523\trs786201005   GENEINFO=ISG15;CLNSIG=5\n",
    "1\t949696\trs672601345   GENEINFO=ISG15;CLNSIG=5\n",
    "1\t949739\trs672601312\t  GENEINFO=ISG15; CLNSIG=0\n",
    "1\t955597\trs115173026\t  GENEINFO=AGRN;CLNSIG=2\n",
    "1\t955619\trs201073369\t  GENEINFO=AGG\n",
    "1\t957640\t.\t  GENEINFO=AGG;CLNSIG=5\n",
    "1\t976059\trs544749044\t  GENEINFO=AGG;CLNSIG=255\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**[Cynthia Hu]:**\n",
    "\n",
    "My goal is to answer the question: what gene and mutation combinations are classified as dangerous. My expected output from the data is a processed data set with columns related to gene and mutation information only and danderous combinations only. Aslo, I will summarize the data, saying show the top 10 dangerous combinations in bar charts.\n",
    "\n",
    "Below are my steps.\n",
    "\n",
    "1. read the data\n",
    "   Read VCF file, tab delimited. use pandas.read_csv()\n",
    "   don't need the first four rows as they are information about the data.\n",
    "   The data has a column names in the fifth row.\n",
    "   \n",
    "2. print several rows to exam\n",
    "\n",
    "3. remove '#' from column name for #CHROM\n",
    "\n",
    "4. understand defintion of each column, refering to the document above\n",
    "\n",
    "5. select variables used for this task: CHROM, POS, ID and INFO. \n",
    "\n",
    "6. split the INFO columns to get separate columns for GENEINFO, CLNSIG and CLNDBN\n",
    "\n",
    "   1) use apply() + lambda funtion to split each row --> output may be a list of list  \n",
    "   2) each variable with key = value format, using dictionary/list to convert it to columns?\n",
    "      loop through each item in the list (nested loops), extract the key and value respectively, using find(\"=\") and len() functions.       \n",
    "   3) not all records with all three variables; may use blank or null for missing values  \n",
    "   4) after testing the steps, then try to combine them to create several columns directly\n",
    "   \n",
    "7. explore the dataset and do sanity-check\n",
    "\n",
    "    1) check several records of the dataframe, head()\n",
    "    \n",
    "    2) how many variables and their data type: shape, dtypes,describe()\n",
    "    \n",
    "    3) which variables have missing values, and what's the percentage of missing records. can calculate from step 2)\n",
    "    \n",
    "    4) distribution of the variables, using pd.value_counts()\n",
    "    \n",
    "8. filter data set from prior step to include records with only dangerous mutation\n",
    "    CLNSIG == 5 (Pathogenic)\n",
    "    Records with missing values for CLNSIG would be excluded from the analysis\n",
    "    \n",
    "9. group the dataset by GENEINFO and ID; use count function to summarize and sort the data descending\n",
    "\n",
    "10. print the grouped data\n",
    "\n",
    "11. transform the data for plotting. reset_index\n",
    "\n",
    "12. show top 10 combinations in a bar chart\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
